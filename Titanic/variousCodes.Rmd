I'm not going to do any further grouping on the fare but will add this feature in my model. 


# 3.7 Data Wrangling Test Data

Since I added some new features to train data, I have to do the same to test data otherwise the predict model will not work. 

```{r}
test$AgeGrp[test$Age<=12] <- 'Kids'
test$AgeGrp[test$Age>12 & test$Age<=55] <- 'Adults'
test$AgeGrp[test$Age>55] <- 'Seniors'
test$AgeGrp <- factor(test$AgeGrp)


test$status <- 'Family'
test$status[test$Parch==0 & test$SibSp==0] <- 'Single'
test$status <- factor(test$status)
test$status <- factor(test$status)

summary(test)
```


```{r}
# now let's apply the model to test data
test$Survived <-  predict(fit.rf,test)
```


# Machine Learning 

After analysing all the features in Titanic dataset, I've finally come up with 5 final features :

  * Pclass
  * AgeGrp
  * FamilySize
  * Fare
  * Sex
  
  

```{r}

set.seed(1)
models <- c("rf", "xgbTree")
fits <- lapply(models, function(model){ 
	print(model)
	train(Survived ~ Sex + Pclass + AgeGrp + FamilySize + Fare, method = model, data = train)
}) 
    
names(fits) <- models
```


Now, I'm going to put all the existing features and new ones to perform 10 fold validation using random forest. The outcome of this is around 84% accuracy 
```{r}
# Machine learning one using 10 fold validation
library(caret)
train$Survived <- factor(train$Survived)


# create 10 fold validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"

# Random Forest
set.seed(7)
fit.rf <- train(Survived ~ Sex + Pclass + AgeGrp + FamilySize + Fare, data=train, method="rf", metric=metric, trControl=control)

# print model
print(fit.rf)
```





```{r}

fit.rf <- train(Survived ~ Sex + Pclass + AgeGrp + FamilySize + Fare, data=train, method="rf", trControl=control, tuneGrid = data.frame(mtry = seq(2, 5, 1)))
ggplot(fit.rf, highlight = TRUE)


fit.rf$bestTune
fit.rf$finalModel
```




Let's review how my prediction performs against training data and review the confusion matrix.
My accuracy is 89.6% and balance accuracy is about 88%. So it is pretty good result and quite balanced in terms of predicting the outcome 1 or 0.


```{r}
# based on the summary above, I decided to use random forest as my model
# let's see confusion matrix against training data to see how accurate it is
# put the result to another field called Survived2 and you prob will see accuracy around 84%, roughly about the same with 10 fold testing above

train$Survived2 <- predict(fit.rf,train)
confusionMatrix(train$Survived2,train$Survived)
```



# fix test data
```{r}
test$AgeGrp[test$Age<=18] <- 'Kids'
test$AgeGrp[test$Age>18 & test$Age<=59] <- 'Adults'
test$AgeGrp[test$Age>59] <- 'Seniors'
test$AgeGrp <- factor(test$AgeGrp)

test$FamilySize <- test$Parch + test$SibSp + 1



```



```{r}
test$Survived <- predict(fit.rf,test)
```


# Submission file to Kaggle to check final score. 
```{r}
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "titanic6.csv", row.names = FALSE)
```


